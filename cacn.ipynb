{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYCZJvJowTNK",
        "outputId": "91d33b4d-78de-402b-948c-164e23bcaff9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"openai version:\", openai.__version__)\n",
        "print(\"csv version:\", csv.__version__)  # Note: csv is a built-in module, it doesn't have a version attribute\n",
        "print(\"re version:\", re.__version__)  # Note: re is a built-in module, it doesn't have a version attribute\n",
        "print(\"nltk version:\", nltk.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqtA0mQBxmWG",
        "outputId": "dc33dade-c132-458f-d19b-f59368a5b4ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas version: 1.5.3\n",
            "openai version: 0.28.0\n",
            "csv version: 1.0\n",
            "re version: 2.2.1\n",
            "nltk version: 3.8.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/drive/MyDrive/Claim Normalization/cacn #path to the repository"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wb8YA3HgnM0",
        "outputId": "92c0663d-df3b-4503-9b88-8dbba97f68ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Claim Normalization/cacn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CACN:\n",
        "    def __init__(self, api_key, prompt_file, data_file, output_file):\n",
        "        self.api_key = api_key\n",
        "        self.prompt = self.get_in_context_examples(prompt_file)\n",
        "        self.data = self.get_data(data_file)\n",
        "\n",
        "    def get_in_context_examples(self, prompt_file):\n",
        "        \"\"\"\n",
        "        Read the In-context examples from the given file\n",
        "\n",
        "            Args: file path\n",
        "            Returns: cleaned in-context examples to append with prompt.\n",
        "        \"\"\"\n",
        "        # Read the prompt from the text file and save as prompt\n",
        "        with open(prompt_file, 'r') as file:\n",
        "            prompt = file.read()\n",
        "            prompt = re.sub(r'[^a-zA-Z0-9.:?\\s]', '', prompt)\n",
        "            prompt = prompt.replace('\\t', ' ').replace('\\n', ' ')\n",
        "\n",
        "        return prompt\n",
        "\n",
        "\n",
        "    def decontract(self, text):\n",
        "        \"\"\"\n",
        "        Decontract the contracted words in the given text.\n",
        "        Args:\n",
        "            text (str): Text containing contracted words.\n",
        "        Returns:\n",
        "            str: Text with contracted words expanded.\n",
        "        Example:\n",
        "            >>> decontract(\"I can't go. It's raining.\")\n",
        "            \"I cannot go. It is raining.\"\n",
        "        \"\"\"\n",
        "        contractions_dict = {}\n",
        "        with open('replacements.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                key, value = line.strip().split(': ')\n",
        "                contractions_dict[key] = value\n",
        "\n",
        "        for contraction, expansion in contractions_dict.items():\n",
        "          pattern = re.compile(contraction, re.IGNORECASE)\n",
        "          text = re.sub(pattern, expansion, text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def clean_post(self, post):\n",
        "        # remove links\n",
        "        post = re.sub(r\"http:\\S+\", \"\", post)\n",
        "        # remove special characters\n",
        "        post = re.sub(r\"[\\(\\)#@!\\^\\\\\\/\\+><]\", \"\", post)\n",
        "        # remove extra white spaces\n",
        "        post = re.sub(r\"\\s+\", \" \", post)\n",
        "        # lower case\n",
        "        post = post.lower()\n",
        "\n",
        "        return post\n",
        "\n",
        "\n",
        "    def get_data(self, data_file):\n",
        "        \"\"\"\n",
        "        Read the data and resturn preprocssed posts.\n",
        "\n",
        "            Args: File path\n",
        "            Returns: Dataframe\n",
        "\n",
        "        Example:\n",
        "            >>> decontract(\"I can't go. It's too late.\")\n",
        "            \"I cannot go. It is too late.\"\n",
        "        \"\"\"\n",
        "\n",
        "        df =  pd.read_csv(data_file)\n",
        "\n",
        "        # preprocess posts\n",
        "        df['clean post']= df['Social Media Post'].apply(self.decontract)\n",
        "        df['clean post'] = df['clean post'].apply(self.clean_post)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def extract_claim(self, sentence):\n",
        "        \"\"\"\n",
        "        Extract normalized claim from the response generated by the model\n",
        "\n",
        "        Args:\n",
        "            sentence (str): The sentence generetaed by gpt\n",
        "\n",
        "        Returns:\n",
        "            str: Normalized claim\n",
        "\n",
        "        Example:\n",
        "            >>> extract_claim(\"The post claims that Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab. This claim is verifiable through various reports and has a huge social impact. Thus, the central claim here is Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab.\")\n",
        "            \"Thailand will ban Pfizer vaccines after a Thai princess falls into a coma following a booster jab.\"\n",
        "        \"\"\"\n",
        "\n",
        "        sentence = sentence.replace(\"U.S.\", \"US\")\n",
        "        sentence = sentence.replace(\"Dr.\", \"Dr\")\n",
        "        sentence = sentence.replace(\"Ph.D.\", \"PhD\")\n",
        "\n",
        "        sentence = nltk.sent_tokenize(sentence)\n",
        "        claim = sentence[-1].strip()   #get last sentence, which contains the central claim from the response.\n",
        "        # print(last_sentence)\n",
        "\n",
        "        pattern = r'(the central claim is|the normalized checkworthy claim is|the crucial checkworthy claim is|claim should be|claim here is|normalized checkworthy claim should be|central checkworthy claim is|normalized checkworthy claim should be:|he central claim in the post is|to be fact-checked here is)(?: that)?(.*?)[\\.\\n]'\n",
        "        matches = re.search(pattern, claim, flags=re.IGNORECASE)\n",
        "        if matches:\n",
        "            claim = matches.group(2).strip()\n",
        "            claim = claim.replace(\":\", \"\")\n",
        "            return claim\n",
        "        return claim\n",
        "\n",
        "    def generate_claims(self):\n",
        "        openai.api_key = self.api_key\n",
        "        SUMM_MAX_LENGTH = 120\n",
        "        MAX_TOKEN_LIMIT = 4096\n",
        "\n",
        "        with open(output_file, 'a', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['Social Media post','CACN Normalized Claim','Gold Normalized Claim'])  #create a new file and write headers\n",
        "\n",
        "\n",
        "            for instance in range(len(self.data)):\n",
        "                text = self.data['clean post'].iloc[instance]\n",
        "                prompt_text = f\"{self.prompt} Identify the central claim in the given post: {text} \\n Let's think step by step.\"\n",
        "\n",
        "                if len(prompt_text) > MAX_TOKEN_LIMIT:\n",
        "                    text = text[:MAX_TOKEN_LIMIT]\n",
        "                    prompt_text = f\"{self.prompt} Identify the central claim in the given post: {text} \\n Let's think step by step.\"\n",
        "\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"text-davinci-003\",\n",
        "                    prompt=prompt_text,\n",
        "                    temperature=0.6,\n",
        "                    max_tokens=SUMM_MAX_LENGTH,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0.1,\n",
        "                    stop=None\n",
        "                )\n",
        "\n",
        "                gpt_summary = response[\"choices\"][0][\"text\"].strip()\n",
        "                normalized_claim = self.extract_claim(gpt_summary)\n",
        "                row = [self.data.iloc[instance]['Social Media Post'], normalized_claim, self.data.iloc[instance]['Normalized Claim']]\n",
        "                print(normalized_claim)\n",
        "                writer.writerow(row)\n",
        "                print(row)"
      ],
      "metadata": {
        "id": "BH-6p6SvgnKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    api_key = <add-your-key-here>\n",
        "    prompt_file = 'prompt.txt'\n",
        "    data_file = 'CLAN-samples.csv'\n",
        "    output_file = 'output.csv'\n",
        "\n",
        "    claim_extractor = CACN(api_key, prompt_file, data_file, output_file)\n",
        "    claim_extractor.generate_claims()"
      ],
      "metadata": {
        "id": "2SIt84PDgnIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wwb8flLFgnFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}